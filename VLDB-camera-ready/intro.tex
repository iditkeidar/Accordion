
\subsection{LSM Stores} 
% NoSQL stores are popular
Persistent NoSQL \emph{key-value stores (KV-stores)} 
have become extremely popular over the last decade, 
and the range of applications for which they are used continuously increases. A small sample of  recently 
published use cases includes massive-scale online analytics (Airbnb/ Airstream~\cite{airbnb}, 
Yahoo/Flurry~\cite{flurry}), product search 
and recommendation (Alibaba~\cite{alibabahbase}), 
graph storage (Facebook/Dragon~\cite{dragon}, 
Pinterest/Zen~\cite{zen}), and many more. 

% LSM stores optimize disk I/O 
The leading approach for implementing write-intensive key-value storage is \emph{log-structured merge (LSM)} stores~\cite{O'Neil1996}.
This technology is ubiquitously used by popular key-value storage platforms~\cite{leveldb, rocksdb, scylladb, Chang2008, cassandra, hbase, mongodb, mysql}. 
%(e.g., LevelDB~\cite{leveldb},  RocksDB~\cite{rocksdb}, ScyllaDB~\cite{scylladb}, Bigtable~\cite{Chang2008}, Cassandra~\cite{cassandra}, HBase, MongoDB~\cite{mongodb}, MySql~\cite{mysql}). 
The premise for using LSM stores is the major disk access bottleneck, exhibited even with today's SSD hardware~\cite{rocksdb,Tanenbaum:2014:MOS:2655363,Wu:2012:AWB:2093139.2093140}. 

An LSM store includes a \emph{memory store} in addition to a large \emph{disk store},
which is comprised of a collection of files
(see Figure~\ref{fig:LSM}). 
The memory store absorbs writes, and is periodically \emph{flushed} to disk as a new immutable file. 
This approach improves write throughput by transforming expensive random-access I/O into storage-friendly sequential I/O. 
Reads search the memory store as well as the disk store. 

The number of files in the disk store has adverse impact on read performance. 
In order to reduce it, the system periodically runs a background \emph{compaction} process, which reads some files from 
disk and merges them into a single sorted one while removing redundant (overwritten or deleted) entries.
We provide further background on LSM stores in Section~\ref{sec:background}.

\subsection{LSM Performance Bottlenecks} 
% Compactions hurt 
The performance of modern LSM stores is highly optimized, and yet as technology scales and real-time 
performance expectations increase, these systems face more stringent  demands. 
In particular, we identify two principal pain points. 

First, 
LSM stores  are extremely sensitive to the rate and extent of compactions. If compactions are infrequent, read performance
suffers (because more files need to be searched and caching is less efficient when  data is scattered across multiple files). On the other hand, if 
compactions are too frequent, they jeopardize the performance of both writes and reads by consuming CPU and I/O resources, and by indirectly invalidating cached blocks. 
\remove{
In other words, LSM stores struggle to balance between {\em read amplification} (the number 
of disk reads per query) and {\em write amplification} (the ratio of bytes written to disk versus bytes written to the 
database)~\cite{rocksdbtuning}. 
}
In addition to their performance impact, frequent compactions increase the disk write volume, which  
accelerates device wear-out, especially for SSD hardware~\cite{Hu:2009}. 

Second, as the amount of memory available in commodity servers rises, LSM stores gravitate towards using larger memory stores.
However, state-of-the-art memory stores are organized as dynamic data structures consisting of many 
small objects, which becomes inefficient in terms of both cache performance and 
\emph{garbage collection (GC)} overhead when the memory store size increases.
%
This has been reported to cause significant problems,  which developers 
  of managed stores like Cassandra~\cite{cassandra} and HBase~\cite{hbase}  have 
had to cope with~\cite{cassandraoffheap,hbasemslab}.  
Note that the impact of GC pauses is aggravated when timeouts are used to detect faulty storage nodes and initiate fail-over.



%Existing solutions - compaction
Given the important role that compaction plays in LSM stores, it is not surprising that 
significant efforts have been invested in compaction parameter tuning, scheduling, and so on~\cite{hbasetuning,
universalcompaction,scylladbcompaction,Sears:2012}. However, these approaches only deal with the aftermath
of organizing the disk store as a sequence of files created upon memory store overflow events, and do not 
address memory management. 
And yet, the amount of data written by memory flushes  
directly impacts the ensuing flush and compaction toll. 
Although the increased size of  memory stores makes flushes less frequent, 
today's LSM stores do not physically delete removed or overwritten data from the memory store. 
%, and do not change the layout of data that is doomed to be immutable while in-memory.
Therefore, they do not reduce the amount of data written to disk.

%Existing solutions - memory layout 
The problem of inefficient memory management in managed LSM stores has received much less attention. 
The only existing  solutions that we are aware of work around the lengthy JVM GC pauses by managing memory allocations on their own, either using pre-allocated object pools and local allocation buffers~\cite{hbasemslab}, or by moving the data to off-heap memory~\cite{cassandraoffheap}. 

\subsection{Accordion} 
% Drumroll 
We introduce \sys, an algorithm for memory store management in LSM stores. 
\sys\ re-applies to the memory store 
the classic LSM  design principles, which were originally used only for the disk store. 
The main insight is that the memory store can be partitioned into two parts: (1) 
 a small dynamic segment that absorbs writes, and (2) a sequence of static segments created 
 from previous dynamic segments. Static segments are created by \emph{in-memory flushes} occurring at a higher rate
 than disk flushes.  
 Note that the key disadvantage of LSM stores -- namely, the need to search multiple files on read -- 
is not as significant in Accordion because  RAM-resident segments can be searched efficiently, possibly 
in parallel.
 
 Accordion takes advantage of the fact that static segments are immutable, and 
 optimizes their index layout as flat; it can further \emph{serialize} them, i.e., remove a level of indirection.
 % having it flat and even serialized.
 The algorithm also performs \emph{in-memory compactions}, which eliminate redundant %(removed or overwritten) 
 data \emph{before} it is written to disk.
\sys's data organization is illustrated in Figure~\ref{fig:accordion};
 Section~\ref{sec:accordion} fleshes out its details.

% Benefits
The new design has the following benefits:
%This benefits of the new design are three-fold:
\begin{itemize} 
%(1) space reduction -> reduces disk compaction write amplification and read latency
 \item \emph{Fewer compactions.}
The reduced footprint of immutable indices as well as in-memory compactions delay disk flushes. In turn, this reduces the write volume (and resulting disk wear) by reducing the amount of disk compactions.
\item \emph{More keys in RAM.} Similarly, the efficient memory organization allows us to keep more keys 
in RAM, which can improve read latency, especially when slow HDD disks are used.
%We note that unlike disk levels in LSM design where read latency suffers from the need to
%search multiple files on disk,  \sys's additional immutable segments reside in RAM and hence can be searched efficiently, even in parallel.
 %(2) gc reduction -> increases throughput
\item \emph{Less GC overhead.}  By dramatically reducing the size of the dynamic segment, \sys\ reduces GC overhead, thus improving write throughput and making performance more predictable.
%(3) flat layout -> cache friendly and off-heap friendly
The flat organization of the static segments 
is also readily amenable to off-heap allocation, which further reduces memory consumption
 by allowing serialization, i.e., eliminating the need to store Java objects for data items. 
\item \emph{Cache friendliness.} Finally, the flat nature of immutable indices improves locality of reference and hence boosts hardware cache efficiency. 
\end{itemize}
 
% Experiments
\sys\ is implemented in HBase production code. In Section~\ref{sec:eval} we experiment with the \sys\ HBase implementation in a range of scenarios.
We study production-size datasets and data layouts, with multiple storage types (SSD and HDD). 
We focus on high throughput scenarios, where the compaction and GC toll are significant. 

Our experiments show that the algorithm's contribution to overall system performance is substantial, 
especially under a  heavy-tailed (Zipf) key access distribution with 
small objects, as occurs in many production use cases~\cite{Wu2015}. For example, \sys\/ 
improves the system's write throughput by up to $48\%$, and reduces read tail latency 
(in HDD settings)  
by up to $40\%$. At the same time, it reduces the write volume (excluding the log) by up to $30\%$. Surprisingly, we see 
that in many settings, disk I/O is \emph{not} the principal bottleneck. Rather, the memory management 
overhead is more substantial: the improvements are highly correlated with  reduction in GC time. 

To summarize, \sys\ takes a proactive approach for handling disk compaction even before data hits the disk, and addresses GC toll by directly improving the memory store structure and management.
It grows and shrinks a sequence of static memory segments resembling accordion bellows, 
thus increasing memory utilization and reducing fragmentation, garbage collection costs, and the disk write volume. 
Our experiments show that it significantly improves end-to-end system performance and reduces disk wear, 
which led to the recent adoption of this solution as HBase's default memory store implementation; it is generally available starting the HBase 2.0 release.
 Section~\ref{sec:related} discusses related work 
and Section~\ref{sec:conclusions} concludes the paper.

