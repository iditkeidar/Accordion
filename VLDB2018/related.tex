




\remove{
In other words, LSM trees struggle to balance between {\em read amplification} (the number 
of disk reads per query) and {\em write amplification} (the ratio of bytes written to disk versus bytes written to the 
database)~\cite{rocksdbtuning}. 
}


Because compaction frequency and efficiency has a significant on LSM tree performance and disk wear, 
many previous works have focused on tuning compaction parameters and scheduling~\cite{hbasetuning,rocksdb,
scylladbcompaction,universalcompaction,Sears:2012} or reducing their cost via more efficient on-disk organization~\cite{wisckey}. 
However, these works focus on \emph{disk compactions},
which deal with data \emph{after} it has been flushed to disk. We are not aware of any previous work that 
added \emph{in-memory} compactions to LSM trees.

Some previous works have focused on other in-memory optimizations. 
cLSM~\cite{clsm} focused on scaling LSM trees on multi-core hardware by adding lightweight synchronization to LevelDB. 
\sys, on the other hand, is based on HBase, which already uses a concurrent in-memory skiplist; improving concurrency
in the line of cLSM is orthogonal to our contribution.  

Similarly to us, the authors of FloDB~\cite{flodb} also observed that using large skiplists is detrimental to 
LSM tree performance, and so FloDB also partitions the memory component. However, rather than using 
a small skiplist and large flat segments as \sys\ does, FloDB  uses a small hash table and a large skiplist.
Put operations inserting data to the small hash table are much faster than insertions to the large skiplist, 
and the latter are batched and inserted via multi-put operations that amortize the cost of the slow skiplist access.
Unlike \sys, FloDB does not reduce the memory footprint by either flattening or compaction. 

LSM-trie~\cite{Wu2015} is motivated by the prevalence of small objects in industrial NoSQL workloads.  
Like us, they observed that skiplists induce high overhead when used to manage many small objects.  
They suggest to replace the LSM tree architecture with an LSM trie, which is based on hashing instead of ordering 
and thus eliminates the memory for indexing. This data structure is much faster, but only for small values, and, more importantly, does not
support range queries.

