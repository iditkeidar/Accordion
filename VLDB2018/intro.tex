
% NoSQL stores are popular
Persistent NoSQL key-value (KV-)stores have become extremely popular over the last decade, 
and the range of applications for which they are used continuously increases. A small sample of  recently 
published use cases includes massive-scale online analytics (Airbnb/ Airstream~\cite{airbnb}, 
Yahoo/Flurry~\cite{flurry}), product search 
and recommendation (Alibaba~\cite{alibabahbase}), 
graph storage (Facebook/Dragon~\cite{dragon}, 
Pinterest/Zen~\cite{zen}), and many more. 

% LSM stores optimize disk I/O 
The leading approach for implementing write-intensive key-value storage is \emph{log-structured merge (LSM)} stores~\cite{O'Neil1996}.
This technology is ubiquitously used by popular key-value storage platforms~\cite{leveldb, rocksdb, scylladb, Chang2008, cassandra, hbase, mongodb, mysql}. 
%(e.g., LevelDB~\cite{leveldb},  RocksDB~\cite{rocksdb}, ScyllaDB~\cite{scylladb}, Bigtable~\cite{Chang2008}, Cassandra~\cite{cassandra}, HBase, MongoDB~\cite{mongodb}, MySql~\cite{mysql}). 
The premise for using LSM stores is the major disk access bottleneck, exhibited even with today's SSD hardware~\cite{rocksdb,Tanenbaum:2014:MOS:2655363,Wu:2012:AWB:2093139.2093140}. 
An LSM store includes a \emph{memory store} in addition to the large \emph{disk store} comprising a collection of files
(see Figure~\ref{fig:LSM}). 
The memory store absorbs writes, and is periodically \emph{flushed} to disk as a new immutable file. 
This approach improves write throughput by transforming expensive random-access I/O into storage-friendly sequential I/O. 
Reads search the memory store as well as the disk store. The number of files in the disk store has adverse impact on read performance. 
In order to reduce it, the system periodically runs a background \emph{compaction} process, which reads some files from 
disk and merges them into a single sorted one while removing redundant (overwritten or deleted) entries.
We provide further background on LSM stores in Section~\ref{sec:background}.

% Compactions hurt 
The performance of modern LSM stores is highly optimized, and yet as technology scales and real-time 
performance expectations increase, these systems face more stringent  demands. In particular, 
they are extremely sensitive to the rate and extent of compactions. If compactions are infrequent, read performance
suffers (because more files need to be searched and caching is less efficient when  data is scattered across multiple files). On the other hand, if 
compactions are too frequent, they jeopardize the performance of both writes and reads by consuming CPU and I/O resources, and by indirectly invalidating cached blocks. 
\remove{
In other words, LSM stores struggle to balance between {\em read amplification} (the number 
of disk reads per query) and {\em write amplification} (the ratio of bytes written to disk versus bytes written to the 
database)~\cite{rocksdbtuning}. 
}
In addition to their performance impact, frequent compactions increase the disk write volume, which  
accelerates device wear-out, especially for SSD hardware~\cite{Hu:2009}. 

Moreover, as the amount of memory available in commodity server is rising LSM stores gravitate towards using larger memory stores.
This has led to unpredictable \emph{garbage collection (GC)} pauses in managed-language stores~\cite{cassandra, hbase} to become a significant problem.
These pauses are crucial, for example, when timeouts are used to detect faulty storage nodes and initiate fail-over.
In particular, state-of-the-art memory stores are organized as dynamic data structures consisting of many small objects,
which become inefficient when the memory store size increases; they are neither cache-friendly nor GC-friendly.

%Existing solutions 
Not surprisingly, significant efforts have been invested in compaction parameter tuning, scheduling, etc.~\cite{hbasetuning,
universalcompaction,scylladbcompaction,Sears:2012}. However, these approaches only deal with the aftermath
of organizing the disk store as a sequence of files created upon memory store overflow events, and do not 
address memory management. 
And yet, the amount of data written by memory flushes  
directly impacts the ensuing flush and compaction toll. 
Memory stores have expanded in size, which make flushes less frequent. 
Memory stores, however, do not physically delete removed or overwritten data, and do not change the layout of data that is doomed to be immutable while in-memory.
Therefore, they do not reduce the amount of data written to disk.

We are not aware of any work that tried to directly tackle the memory management problem in LSM stores. 
Two widely used technique to work around lengthy JVM GC pauses when working with large heap sizes are 
(1) using pre-allocated object pools and local allocation buffers~\cite{hbasemslab}, and 
(2) moving the data into off-heap memory~\cite{cassandraoffheap}. 

% Drumroll 
We introduce \sys, an algorithm for memory store management in LSM stores. 
\sys\ re-applies the classic LSM store design principles to the memory store. The main insight is that the memory store can be partitioned
 into a small dynamic segment that absorbs writes, and a sequence of static segments created 
 from previous dynamic segments. Static segments are created by \emph{in-memory flushes} occurring at a higher rate
 than disk flushes.  The algorithm takes advantage of the fact that static segments are immutable and hence can optimize their index layout having it flat and even serialized.
 The algorithm also performs \emph{in-memory compactions}, which eliminate redundant %(removed or overwritten) 
 data before it is written to disk.
\sys's data organization is illustrated in Figure~\ref{fig:accordion};
 Section~\ref{sec:accordion} fleshes out its details.

% Benefits
This benefits of the new design are three-fold:
\begin{itemize} 
%(1) space reduction -> reduces disk compaction write amplification and read latency
 \item First, the reduced footprint of immutable indices as well as in-memory compactions delay disk flushes. In turn, this reduces 
 the write volume by reducing the amount of disk compactions  and improves read latency as more reads can be satisfied from RAM.
We note that unlike disk levels in LSM design where read latency suffers from the need to
search multiple files on disk,  \sys's additional immutable segments reside in RAM and hence can be searched efficiently, even in parallel.
 %(2) gc reduction -> increases throughput
\item Second, by dramatically reducing the size of the dynamic segment  \sys\ reduces GC overhead, thus improving write throughput and making performance more predictable.
%(3) flat layout -> cache friendly and off-heap friendly
\item Finally, the flat nature of immutable indices improves locality of reference and hence boosts hardware cache efficiency. 
This flat organization is also readily amenable to off-heap allocation, which further reduces memory consumption
 by eliminating the need to store Java objects for data items. 
\end{itemize}
 
% Experiments
\sys\ is implemented in HBase production code. In Section~\ref{sec:eval} we experiment with the \sys\ HBase implementation in a range of scenarios.
We study production-size datasets and data layouts, with multiple storage types (SSD and HDD). 
We focus on high throughput scenarios, where the compaction and GC toll are significant. 
We conclude that the algorithm's contribution to overall system performance is substantial, 
especially under a  heavy-tailed (Zipf) key access distribution with 
small objects, as occurs in many production use cases~\cite{Wu2015}. For example, \sys\/ 
improves the system's write throughput by up to $48\%$, and reduces read tail latency 
(in HDD settings)  
by up to $40\%$. At the same time, it reduces the write volume (excluding the log) by up to $30\%$. Surprisingly, we see 
that in many settings, disk I/O is \emph{not} the principal bottleneck. Rather, the memory management 
overhead is more substantial: the improvements are highly correlated with  reduction in GC time. 

To summarize, \sys\ takes a proactive approach for handling disk compaction even before data hits the disk, and addresses GC toll by directly improving the memory store structure and management.
It grows and shrinks a sequence of static memory segments resembling accordion bellows, 
thus increasing memory utilization and reducing fragmentation, garbage collection costs, and the disk write volume. 
Our experiments show that it significantly improves end-to-end system performance and reduces disk wear, 
which led to the recent adoption of this solution as HBase's default memory store implementation; it is generally available starting the HBase 2.0 release.
 Section~\ref{sec:related} discusses related work 
and Section~\ref{sec:conclusions} concludes the paper.

