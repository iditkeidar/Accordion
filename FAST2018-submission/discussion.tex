
While disk compactions in LSM trees have gotten a lot of attention, their in-memory organization was, by and large,  ignored.
In this work, we showed that applying the LSM tree principles also to RAM can significantly improve performance and reduce disk wear. 
We presented \sys, a new memory organization for LSM trees, which employs \emph{in-memory} flushes and 
compaction to reduce the LSM tree's memory footprint and improve the efficiency of memory management and access. 
We integrated \sys\ in Apache HBase, where it became the production default following extensive testing. 

In this paper, we evaluated \sys\ with different storage technologies (HDD and SSD) and various compaction policies, leading
to some interesting insights and surprises:

\paragraph{Flattening and active component size}
We showed that flattening the in-memory index reduces the memory management overhead as well as the frequency of
disk writes, thus  improving performance and also reducing disk wear-out. Originally, we expected  an active segment
(skiplist) comprising 10--20\% of the memory store to be effective, and anticipated that smaller active components would 
result in excessive flush and compaction overhead. Surprisingly, we found that performance is improved by using a much 
smaller active component, taking up only 2\% of the memory store. This is due in part to the fact that our workload is comprised
of small objects -- which are common in production workloads~\cite{Wu2015} -- where the indexing overhead is substantial.   

\paragraph{Impact of memory management on write throughput}
We showed that the write volume can be further reduced, particularly in production-like self-similar key access distributions, by merging memory-resident data, namely, eliminating redundant objects.
We expected this reduction in write volume to also improve performance. Surprisingly, we found that 
disk I/O was not a principal bottleneck, even on HDD. In fact, write throughput is strongly correlated with GC time, 
regardless of the write volume.

\paragraph{Cost of redundant data elimination}
We expected redundant data elimination to favorably affect performance, since it frees up memory and reduces the write volume.
This has led us to develop \eager, an aggressive policy that frequently performs in-memory data merges. 
Surprisingly, this proved detrimental for performance. The cost of the \eager\ policy was exacerbated by the 
fact that we benefitted from a significantly smaller active component than originally expected (as noted above),  
where it literally could not keep up with the in-memory flush rate. 
This led us to develop a new policy, \adp, which heuristically decides when to perform data merges based on 
an estimate of the expected yield. 
While \adp\ does not improve performance compared to the the \basic\ policy, which merges only indices and not data,
it does reduce the write volume, which is particularly important for longevity of SSD drives.
 
\paragraph{Future direction: flattening and off-heap memory}
Our implementation is strictly within the Java-based HBase framework, using Java's standard memory allocator.
This implies that our flattening, while reducing skiplist indices, does not remove redundant indirections altogether.
Specifically, our flat segment is an array of pointers to objects, which point to data cells. 
By replacing the Java memory management with custom HBase slab allocator 
(MSLAB~\cite{hbasemslab}), we can do away with this intermediate level of indirection, 
and allocate the static segment indices off-heap. Offloading the data from the Java heap
has been shown to be effective for read traffic~\cite{alibabahbase}. 
Our preliminary experiments show that it is promising  for writes as well.  

