
% NoSQL stores are popular
NoSQL databases such as Apache HBase~\cite{hbase} 
have become extremely popular over the last decade, and the range of applications for which they are used continously increases. 
For example, in a recent HBase Conference (HBaseCon)\footnote{\url{http://hbase.apache.org/www.hbasecon.com}}, 
the following companies reported using HBase -- 
Yahoo Flurry for mobile app analytics; Yahoo for mail metadata and search; Pinterest for user data and graph services; 
Alibaba for chat and online trade data; and Facebook for messaging. Similarly, 
Imgur uses HBase to power its notifications system\footnote{\url{https://dzone.com/articles/why-imgur-dropped-mysql-in-favor-of-hbase}};
Spotify uses HBase as base for Hadoop and machine learning
jobs\footnote{\url{https://apachebigdata2015.sched.com}};
and the list goes on. 

% LSM trees optimize disk i/o 
The leading approach for implementation of large NoSQL databases is using 
 \emph{Log-structured merge (LSM)} trees, which are employed by  
Apache HBase, Bigtable, RocksDB, LevelDB, MongoDB, SQLite4, WiredTiger, Apache Cassandra,  InfluxDB, and many more.
The premise for using LSM trees is that disk access is considered the 
principal bottleneck in storage systems, even with today's SSDs~\cite{rocksdb,Tanenbaum:2014:MOS:2655363,Wu:2012:AWB:2093139.2093140}.
Since reads can typically be masked by caching, the main design goal is to improve write throughput, which LSM trees do by batching 
writes in memory and periodically \emph{flushing} the  memory  to disk using sequential I/O.
An LSM tree thus includes a \emph{memory store} in addition to the large  \emph{disk store} consisting of a collection of files. 
The memory store absorbs writes and is periodically flushed to disk as a new file. 
To reduce the number of files, the data store periodically runs a \emph{compaction} process, which reads some files from the disk store
and merge-sorts them into a single file while removing redundant (deleted or overwritten) entries. 
For further background on %HBase and 
LSM trees, see Section~\ref{sec:background}.

% Compactions hurt 
The performance of state-of-the-art LSM stores is highly tuned, and yet as technology scales 
and real-time performance expectations from Internet applications increase, these systems face more stringent performance demands.
LSM performance highly depends on the rate and extent of compactions it performs. 
Compactions speed up reads by reducing the number of files the read has to search in, but slow down writes by consuming 
CPU and disk resources. They also increase disk wear.    
%One of the pain points with LSM stores is compactions \Idit{Can we cite something from RocksDB or another KV store here?}.
In HBase, a major pain point is the occurrance of so-called \emph{major compactions}, which replace all the files of a given table.
Indeed, HBase manuals and tuning tips are often aimed at reducing or even avoiding major compactions altogether.
 %\footnote{\url{https://blogs.msdn.microsoft.com/ashish/2016/09/02}}.

% Memory management
LSM trees can benefit from a large memory store, which absorbs many writes before each disk flush, 
and in turn delays the need for disk compactions, \Idit{Citations for this?} including major ones. 
The memory store is \emph{dynamic}, i.e., constantly changes, and must support fast lookup by key. 
It is therefore typically organized as a skip list
\Idit{Citations for this?} or a similar data structure.  Unfortunately, managing a large dynamic data structure induces 
non-negligble  overhead. 
Moreover, 
LSM-based data stores typically perform compactions only on disk and do not attempt to eliminate redundancies in the memory store.
Given that data access patterns often follow heavy-tailed distributions like Zipf,  this means that many versions of popular objects are 
wastefully kept in the in-memory data structure. 
Our work is motivated by this management overhead and waste. 

% Drumroll 
In Section~\ref{sec:accordion}
we introduce \sys, a new memory management scheme for LSM trees.
The principles behind \sys\ are 
\begin{enumerate}
\item \emph{in-memory compactions} can free up memory space to absorb more writes before the
memory store needs to be flushed, thus reducing compactions and disk wear; and
\item \emph{flattening} dynamic data structures into static buffers can reduce memory management overhead.
\end{enumerate}

% Implementation
In Section~\ref{sec:hbase} we  discuss on our implementation of  \sys\ in HBase, which is committed  as the default path in HBase 2.0. 
\Idit{Say something interesting about this process?}
% Evaluation
In Section~\ref{sec:eval} we experiment with \sys\ in a range of  scenarios -- with SSD and HDD storage, with different numbers
of regions, and different workloads. Surprisingly, we see that in virtually all settings, disk I/O is \emph{not} the principal performance bottleneck,
but rather memory management overhead is more substantial. By flattening the memory store \sys\ reduces this overhead and improves overall performance by \inred{XX\% to XX\%}. In-memory compactions have minor impact on performance at best, but do reduce 
write amplification by \inred{XX\% to XX\%}, which is important for extending disk life.

\Idit{Need to think of a good summary sentence for here. }




