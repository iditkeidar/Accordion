
% NoSQL stores are popular
NoSQL key-value data stores such as Apache HBase~\cite{hbase} have become extremely popular over the last decade, 
and the range of applications for which they are used continously increases. A small sample of the recently 
published use cases includes mass-scale online analytics (Airbnb/Airstream\footnote{\small{\url{https://www.slideshare.net/HBaseCon/apache-hbase-at-airbnb}}}, 
Yahoo/Flurry\footnote{\small{\url{https://www.slideshare.net/HBaseCon/hbasecon-2015-hbase-operations-in-a-flurry}}}), e-commerce product search 
and recommendation (Alibaba~\cite{alibabahbase}), 
graph storage (Facebook/Dragon\footnote{\small{\url{https://code.facebook.com/posts/1737605303120405/dragon-a-distributed-graph-query-engine/}}}, 
Pinterest/Zen\footnote{\small{\url{https://www.slideshare.net/InfoQ/zen-pinterests-graph-storage-service}}}), etc. 

%Similarly, Imgur uses HBase to power its notifications system\footnote{\url{https://dzone.com/articles/why-imgur-dropped-mysql-in-favor-of-hbase}};
%Spotify uses HBase as base for Hadoop and machine learning
%jobs\footnote{\url{https://apachebigdata2015.sched.com}};
%web mail metadata and search serving (Yahoo Mail), 
%\footnote{\small{HBaseCon 2017 -- \url{http://hbase.apache.org/www.hbasecon.com}}}. 

% LSM trees optimize disk i/o 
The leading approach for implementation of write-intensive key-value storage is \emph{log-structured merge (LSM)} trees~\cite{O'Neil:1996}.
This technology is ubiquitously used by popular key-value storage libraries (e.g., LevelDB\footnote{\small{\url{https://github.com/google/leveldb}}}, 
RocksDB\footnote{\small{\url{https://rocksdb.org}}}, ScyllaDB\footnote{\small{\url{https://github.com/scylladb/scylla}}}) and distributed systems on top 
of them (e.g., Bigtable~\cite{Chang2008}, Cassandra\footnote{\small{\url{http://cassandra.apache.org/}}}, HBase, 
MongoDB\footnote{\small{\url{http://mongorocks.org/}}}, MySql\footnote{\small{\url{http://myrocks.io/}}}), etc. 
%which are employed by  Apache HBase, Bigtable, RocksDB, LevelDB, MongoDB, SQLite4, WiredTiger, Apache Cassandra,  InfluxDB, and many more.
The premise for using LSM trees is that disk access is considered the principal bottleneck in storage systems, even with today's SSD hardware~\cite{rocksdb,Tanenbaum:2014:MOS:2655363,Wu:2012:AWB:2093139.2093140}. 
The main design goal is to improve write throughput, which LSM trees do by batching writes in memory 
and periodically \emph{flushing} the memory  to disk. This way, expensive random-access I/O is transformed 
into storage-friendly sequential I/O. 

An LSM tree includes a \emph{memory store} in addition to the large \emph{disk store} consisting of a collection of files. 
The memory store absorbs writes, and is periodically flushed to disk as a new immutable file. Reads search for the data
in the memory store, as well as in the disk store. The number of files has adverse impact on read performance. 
In order to reduce it, the system peridically runs a background \emph{compaction} process, which reads some files from 
the disk and merges them into a single file while removing redundant (overwritten or deleted) entries.
% For further background, see Section~\ref{sec:background}.

% Compactions hurt 
The performance of modern LSM stores is highly optimized, and yet as technology scales and real-time 
performance expectations increase, these systems face more stringent performance demands. In particular, 
they are extremely sensitive to the rate and extent of compactions. If compactions are infrequent, read performance
suffers (e.g., caching is less efficient when the data is scattered across multiple files). On the other hand, if 
compactions are too frequent, they jeopardize the performance of both writes and reads by consuming CPU 
and I/O resources. In formal terms, LSM trees struggle to balance between {\em read amplification} (the number 
of disk reads per query) and {\em write amplification} (the ratio of bytes written to disk versus bytes written to the 
database). In addition to its performance impact, write amplification accelerates device wearout, especially for SSD 
hardware~\cite{Hu:2009}. 

Enormous effort has been invested in designing compaction parameter tuning, scheduling, etc.~\cite{hbasetuning,
universalcompaction,scylladbcompaction,Sears:2012}. However, all these approaches only deal with the aftermath
of organizing the disk store as a sequence of files created upon memory store overflow events. Obviously, the more 
frequently the memory overflows, the bigger the ensuing flush and compaction toll, and the lower the overall 
system performance. Buying more memory alleviates the problem but cannot solve it completely for large datasets. 
The RAM of a typical production machine is 2-3 orders of magnitide smaller than disk space. Therefore, throughout 
its lifetime, a store is flushed to disk hundreds to thousands of times, which entails many more compactions of varying 
extents.  Our work is motivated by stretching the time between flushes without extra hardware cost, 
through better use of memory. 

% The problem
A memory store is an ordered map with {\em get} (point query), {\em scan} (range query) and {\em put} 
(point update) and {\em delete} API's. Traditionally, it is implemented as a dynamic index over a collection of data items. 
(e.g., HBase uses the standard Java concurrent skiplist\footnote{\small{\url{https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ConcurrentSkipListSet.html}}}).
The data is multi-versioned, i.e., every put creates a new immutable version of the data item it is applied to. 
This implementation suffers from two drawbacks. First, the use of a big dynamic data structure entails 
the abundance of auxiliary small objects and references, which inflate the in-memory index compared 
to its compact on-disk sibling, sorted array. The overhead is most significant when the managed objects
are small, i.e., the metadata-to-data ratio is big~\cite{Wu2015}. Second, the versioning mechanism makes 
no attempt to eliminate redundancies prior to flush, i.e., the store grows monotonically, independent in the workload. 
This is especially wasteful for heavy-tailed distributions that are prevalent in production workloads~\cite{Devineni:2015}.

% Drumroll 
We introduce \sys, an algorithm for memory store management in LSM trees. %, which addresses these problems. 
\sys\/ re-applies the classic LSM tree design principles to the memory store. Namely, it partitions this store
 into a small dynamic segment that absorbs the recent writes, and a sequence of static segments created 
 from previous dynamic segments by {\em in-memory flushes}. These segments have an optimized memory
 layout (flat index), which reduces the memory overhead. Depending on the workload, the algorithm may 
 perform {\em in-memory compactions}, which eliminate the redundant versions. Section~\ref{sec:accordion}
 fleshes out the implementation details. 
 
 In-memory flushes and compactions occur at a higher rate than their system-level siblings. They allow the system 
 flush to disk less frequently, and consequently reduce the overall I/O and the CPU cycles wasted on background maintenance
  of the LSM tree structure. In Java implementations (e.g., HBase), shrinking 
 the dynamic component results in singnificant reduction of GC overhead. Furthermore, since the static indices 
 are flat, the system can place them in off-JVM heap memory, thereby exploiting more RAM. Finally, reducing the 
 disk I/O decelerates the hardware wearout period. 

% Status
\sys\/ is implemented in HBase production code; it is generally available starting the HBase 2.0 release. 
The extensive evaluation process led to \sys's adoption as default memory store implementation. 

% Experiments
In Section~\ref{sec:eval} we experiment with \sys\ implementation in HBase in a range of scenarios.
We study production-size datasets and data layouts, as well as multiple storage types (SSD and HDD). 
We conclude that the algorithm's contribution to the overall system performance is substantial, 
especially when working with small objects and heavy-tailed (Zipf) distributions. For example, \sys\/ 
improves the system write throughput by up to $40\%$, and reduces the tail read latency by up to 
\inred{XX\%}. In parallel, it reduces the write amplification by up to \inred{XX\%}. Surprisingly, we see 
that in most settings, disk I/O is \emph{not} the principal bottleneck. Rather, the memory management 
overhead is more substantial: the improvements are highly correlated with GC reduction. 

\sys\/ got its name for periodically growing and shrinking the system's memory footprint, similarly 
to accordion bellows. As our experiments show, this movement delivers fresh air, reduces congestion, 
and improves the end-to-end system performance. 


